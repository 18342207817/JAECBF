<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link href="main.css" rel="stylesheet" media="all">
    <meta name="description" content="Joint-AEC-Beamformer" />
    <meta name="keywords" content="acoustic echo cancellation, muti-channel processing, deep neural networks, beamforming, speech enhancement">
    <script>
    function buttonSwitch(id, text) {
        old_src = document.getElementById(id).src;
        ind = old_src.lastIndexOf('/');
        document.getElementById(id).src = old_src.substr(0, ind + 1) + text;
    }
    </script>
    <title>Joint-AEC-and-Beamforming</title>
</head>




<body>
    <div id="top_arrow" style="position: fixed; bottom: 10px; right: 10px;">
        <a href="#title"><img src="./figures/top_arrow.jpg" style="border: 0pt none ; width: 26px; height: 26px;"/></a>
    </div>
    
    <!-- <p  style="margin-bottom:3cm;">"  "</p> -->
    <h2 id="title" class="auto-style1"><center> JOINT AEC AND BEAMFORMING with Double-Talk Detection using RNN-Transformer </center></h2>
    <p class="auto-style7" align="center">
        <a href="https://github.com/vkothapally" target="_blank">Vinay Kothapally</a>
        <sup>1</sup> &nbsp;&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=nCmKPM4AAAAJ&hl=en&oi=ao" target="_blank">Yong Xu</a>
        <sup>2</sup> &nbsp;&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=pxFJcEEAAAAJ&hl=en&oi=ao" target="_blank">Meng Yu</a>
        <sup>3</sup> &nbsp;&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=4nGncN4AAAAJ&hl=en" target="_blank">Shi-Xiong Zhang</a>
        <sup>4</sup> &nbsp;&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=tMY31_gAAAAJ&hl=en" target="_blank">Dong Yu</a>
        <sup>5</sup> &nbsp;&nbsp;&nbsp;
    </p>
    <p class="auto-style7" align="center">
        Center for Robust Speech Systems (CRSS), The University of Texas at Dallas, Texas, USA<sup>1</sup>, &nbsp;
        Tencent AI Lab, Bellevue, WA, USA<sup>2,3,4,5</sup> 
    </p>


    <!--<p class="auto-style7"  align="center">&nbsp;&nbsp;&nbsp; </p>-->
    <p align=left>&nbsp;</p>
    <p align="center">

        <!-- Image with description -->
        <table style="width:1250px" align="center">
            <tr><td><center><img width=1250px alt="" src="figures/proposed_network.png"></center></td></tr>
            <tr><td><p align="center" class="auto-style5-j">Figure 1. Overview of all deep learning based joint echo cancellation and beamformer trained using time-domain scale-invariant SNR.</p></td></tr>
        </table>

        <!-- Abstract -->
        <p class="auto-style5">&nbsp;</p>
        <p class="style2", style="color:blue;"><strong><span class="auto-style6"><u>Abstract</u></span></strong></p>
        <p align="justify" class="auto-style5-j">
          Acoustic echo cancellation (AEC) is a technique used in full-duplex communication systems to eliminate acoustic feedback of far-end speech. However, their performance degrades in naturalistic environments due to nonlinear distortions introduced by the speaker, as well as background noise, reverberation, and double-talk scenarios. To address nonlinear distortions and co-existing background noise, several deep neural network (DNN)-based joint AEC and denoising systems were developed. These systems are based on either purely ``black-box'' neural networks or ``hybrid'' systems that combine traditional AEC algorithms with neural networks. We propose an all-deep-learning framework that combines multi-channel AEC and our recently proposed self-attentive recurrent neural network (RNN) beamformer. We propose an all-deep-learning framework that combines multi-channel AEC and our recently proposed self-attentive recurrent neural network (RNN) beamformer. Furthermore, we propose a double-talk detection transformer (DTDT) module based on the multi-head attention transformer structure that computes attention over time by leveraging frame-wise double-talk predictions. Experiments show that our proposed method outperforms other approaches in terms of improving speech quality and speech recognition rate of an ASR system.
        </p>


        <!-- Key Contributions -->
        <p class="auto-style5">&nbsp;</p>
        <p class="style2", style="color:blue;"><strong><span class="auto-style6"><u>Key contributions</u></span></strong></p>
        <p align="justify" class="auto-style5-j">
        Our main contributions towards the proposed all-deep-learning (ADL) Joint AEC and Beamforming system are summarized as follows:
        <ol>
          <li>We propose using a joint spatial covariance matrix computed using microphone signals and far-end speech as input features, which accounts for cross-correlation between far-end speech and multiple microphones, essential for designing an efficient multi-channel AEC system.</li>
          <li>We extend our recently proposed generalized spatio-temporal RNN beamformer (GRNNBF) to a joint spatio-temporal RNN AEC beamformer (JRNN-AEC-BF) for handling AEC and beamforming simultaneously using original and AEC processed signals.</li>
          <li>We propose a double-talk detection transformer (DTDT) module based on the multi-head attention transformer structure, that computes attention over time while leveraging double-talk detection to suppress far-end residuals.</li>
        </ol>
        </p>

        
        <!-- Results -->
        <p class="auto-style5">&nbsp;</p>
        <p class="style2", style="color:blue;"><strong><span class="auto-style6"><u>Results on speech quality metrics and word error rate (WER)</u></span></strong></p>
        <p align="justify" class="auto-style5-j">
        <center><img src="results/Results_Table_AEC_BF.png" width="940" /></center>
        </p>

        
        <!-- Pyhton Packages -->
        <p class="auto-style5">&nbsp;</p>
        <p class="style2", style="color:blue;"><strong><span class="auto-style6"><u>Python Scripts</u></span></strong></p>
        <ol>
          <li>Proposed Joint AEC Beamformer model script (<a href="https://github.com/vkothapally/JOINT-AEC-AND-BEAMFORMING/blob/main/src/model.py" target="_blank">model.py</a>)</li>
          <li>Run "python model.py" to print architectural details for the proposed netowrk
        </ol>

        <!-- Performance -->
        <p class="auto-style5">&nbsp;</p>
        <p id="performance" , class="auto-style4", style="color:blue;"><strong><u>Enhanced audio samples from various Systems in the study</u></strong></p>

        <center>
        <table width="1200" border="1">
          <td width="320">
            <p><center><img src="specs/01_No_processing.png" height="271" /></center></p>
            <p><center><audio width="315" controls><source src="audio/01_No_Processing.wav" type="audio/wav" >Your browser does not support the audio element.</audio></center></p>
            <p><center><strong>No Processing</strong></center></p>
          </td>
          <td width="320">
            <p><center><img src="specs/02_JRNNBF.png" height="271" /></center></p>
            <p><center><audio width="315" controls><source src="audio/02_JRNNBF.wav" type="audio/wav" >Your browser does not support the audio element.</audio></center></p>
            <p><center><strong>JRNN-BF</strong></center></p>
          </td>
        </table>
        </center>
        

        <center>
        <table width="1200" border="1">
          <td width="320">
            <p><center><img src="specs/03_SpeexDSP_JRNN_AEC_BF.png" height="271"  /></center></p>
            <p><center><audio width="315" controls><source src="audio/03_SpeexDSP_JRNN_AEC_BF.wav" type="audio/wav" >Your browser does not support the audio element.</audio></center></p>
            <p><center><strong>SpeexDSP + JRNN-AEC-BF</strong></center></p>
          </td>
          <td width="320">
            <p><center><img src="specs/04_FTLSTM_JRNN_AEC_BF.png" height="271"  /></center></p>
            <p><center><audio width="315" controls><source src="audio/04_FTLSTM_JRNN_AEC_BF.wav" type="audio/wav" >Your browser does not support the audio element.</audio></center></p>
            <p><center><strong>FT-LSTM + JRNN-AEC-BF </strong></center></p>
          </td>
        </table>
        </center>
        

        <center>
        <table width="1200" border="1">
          <td width="320">
            <p><center><img src="specs/05_Proposed.png" height="271" /></center></p>
            <p><center><audio width="315" controls><source src="audio/05_Proposed.wav" type="audio/wav" >Your browser does not support the audio element.</audio></center></p>
            <p><center><strong>Proposed JRNN-AEC-BF-DTDT</strong></center></p>
          </td>
          <td width="320">
            <p><center><img src="specs/06_Near_End_Reference.png" height="271" /></center></p>
            <p><center><audio width="315" controls><source src="audio/06_Near_End.wav" type="audio/wav" >Your browser does not support the audio element.</audio></center></p>
            <p><center><strong>Near-End Reverberant Speech</strong></center></p>
          </td>
        </table>
        </center>

        <p><strong>----------------------------------------------------------------------------------------------------------------------------------------------------</strong></p>
        <p><strong>More audio samples can be released upon approval of the submission and company's sharing policy.</strong></p>
        <p><strong>----------------------------------------------------------------------------------------------------------------------------------------------------</strong></p>
        
          
        

        

        <p class="auto-style5">&nbsp;</p>
        <p id="reference" , class="auto-style4", style="color:blue;"><strong><u>References</u></strong></p>
        <p id="ref_1" class="auto-style5">
            [1] Q. Wang and et al., “A frequency-domain nonlinear echo processing algorithm for high quality hands-free voice communication devices,” Multimedia Tools and Applications, vol. 80, no. 7, 2021.
            <br> [2] Y. Park and et al., “Frequency domain acoustic echo suppression based on soft decision,” IEEE Signal Processing Letters, vol. 16, no. 1, pp. 53–56, 2008.
            <br> [3] S. Zhang and et al., “F-T-LSTM Based Complex Network for Joint Acoustic Echo Cancellation and Speech Enhancement,” in Interspeech, 2021, pp. 4758–4762.
            <br> [4] I. Amir and et al., “Nonlinear Acoustic Echo Cancellation with Deep Learning,” in Interspeech, 2021.
            <br> [5] J. Valin, “Speex: A free codec for free speech,” ArXiv, vol.abs/1602.08668, 2016.
            <br> [6] L. Ma and et al., “Acoustic echo cancellation by combining adaptive digital filter and recurrent neural network,” arXiv preprint arXiv:2005.09237, 2020.
            <br> [7] L. Ma and et al., “Echofilter: End-to-end neural network for acoustic echo cancellation,” arXiv preprint arXiv:2105.14666, 2021.
            <br> [8] X. Zhou and et al., “Residual acoustic echo suppression based on efficient multi-task convolutional neural network,” arXiv preprint arXiv:2009.13931, 2020.
            <br> [9] C. Zhang and et al., “A robust and cascaded acoustic echo cancellation based on deep learning.,” in Interspeech, 2020, pp. 3940–3944.
            <br> [10] H. Zhang and et al., “A Deep Learning Approach to Multi-Channel and Multi-Microphone Acoustic Echo Cancellation,” in Interspeech, 2021, pp. 1139–1143.
            <br> [11] L. Ma and et al., “Multi-scale attention neural network for acoustic echo cancellation,” arXiv preprint arXiv:2106.00010, 2021.
            <br> [12] R. Peng and et al., “Acoustic Echo Cancellation Using Deep Complex Neural Network with Nonlinear Magnitude Compression and Phase Information,” in Interspeech, 2021, pp.4768–4772.
            <br> [13] M. M. Halimeh and et al., “Combining adaptive filtering and complex-valued deep postfiltering for acoustic echo cancellation,” in ICASSP, 2021, pp. 121–125.
            <br> [14] Y. Xu and et al., “Generalized spatio-temporal rnn beamformer for target speech separation,” Interspeech, 2021.
            <br> [15] Zhuohuang Zhang, Yong Xu, and et al., “ADL-MVDR: All deep learning MVDR beamformer for target speech separation,” in ICASSP, 2021, pp. 6089–6093.
            <br> [16] W. Mack and et al., “Deep filtering: Signal extraction and reconstruction using complex time-frequency filters,” IEEE Signal Processing Letters, vol. 27, pp. 61–65, 2019.
			<br> [17] M. M. Halimeh and et al., “Combining adaptive filtering and complex-valued deep postfiltering for acoustic echo cancella-tion,” in ICASSP, 2021, pp. 121–125.
        </p>
        <p class="auto-style1"><font color="#999999">Last update: Jan. 6, 2021</font></p>
</body>

</html>